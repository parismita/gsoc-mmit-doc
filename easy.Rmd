---
title: " Train and test a decision tree model"
author: "Parismita Das"
date: "10 January 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

To run train and test a decision tree model using rpart and partykit. And Using 5-fold cross-validation to compare the learned decision tree models to a trivial baseline learner.

## Using rpart
Importing libraries
```{r lib, echo=FALSE}
library(rpart)
library(rpart.plot)
library(partykit)
library(caret)
```

The dataset using is segmentationData of caret package. The following code seperates test and train data and removes "Case" column from dataset.

```{r data, echo=TRUE}
data("segmentationData", package = "caret")
segmentationData$Cell<-NULL
train<-subset(segmentationData,Case=="Train")
test<-subset(segmentationData,Case=="Test")
train$Case<-NULL
test$Case<-NULL
```

In the dataset, the target data to pe predicted is "Class" with two classes as "PS" and "WS". The formula required for creating tree using rpart consist of all features expt y,time,status as given below

```{r form, echo=TRUE}
form <- Class ~ . - Class
```


###The trivial baseline

Creating trivial desicion tree, keeping xval=0 signifies no cross-validation implemented.  

```{r tree1, echo=TRUE}
ctrl <- rpart.control(minsplit = 15, minbucket = 5, xval=0)
tree <- rpart(formula = form, data = train, na.action = na.rpart, control = ctrl, method = "class")
```

The desicion tree formed is:

```{r tree2, echo=FALSE}
rpart.plot(tree)
```

Predicting the target class and calculating the missclassification error:

```{r pred,echo=TRUE}
out<-predict(tree,test)
status_predicted<- colnames(out)[max.col(out, ties.method = c("first"))] # predicted
status_input<- as.character (test$Class) # actuals
mn<- mean (status_input != status_predicted) # misclassification %
```

Hence the error of misclassification is:

```{r pred2,echo=FALSE}
print(paste("error: ",toString(mn*100),"%"))
```

###Desicion tree model using cross validation
Creating desicion tree, using xval=5 which means 5-fold cross-validation implemented.  

```{r tree3, echo=TRUE}
ctrl <- rpart.control(minsplit = 15, minbucket = 5, xval=5)
tree <- rpart(formula = form, data = train, na.action = na.rpart, control = ctrl, method = "class")
```

The complexity parameter (cp) is used to control the size of the decision tree and to select the optimal tree size. Hence plotting cross validation error vs cp 

```{r tree6, echo=FALSE}
plotcp(tree)
printcp(tree)
```

Pruning the tree using cross validation to prevent overfitting, Thus inappropriate nodes are removed

```{r tree7, echo=TRUE}
pdtree<- prune(tree, cp=tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"])
```

```{r plot2,echo=FALSE}
rpart.plot(pdtree)
```

The plots of the r-square (apparent and apparent - from cross-validation) versus the number of splits. And the Relative Error(cross-validation) +/- 1-SE from cross-validation versus the number of splits.
```{r pred5,echo=FALSE}
par(mfrow=c(1,2))
rsq.rpart(pdtree)
par(mfrow=c(1,1))
```

Predicting the target class and calculating the missclassification error for pruned tree:

```{r pred3,echo=TRUE}
out<-predict(pdtree,test)
status_predicted<- colnames(out)[max.col(out, ties.method = c("first"))] # predicted
status_input<- as.character (test$Class) # actuals
mn<- mean (status_input != status_predicted) # misclassification %
```

Hence the error of misclassification is:

```{r pred4,echo=FALSE}
print(paste("error: ",toString(mn*100),"%"))
```

Hence the misclassification error decreased on using k-fold cross-validation. 

## Using partykit
Using conditional trees to train the desicion tree model

```{r ptk1, echo=TRUE}
ctrl <- ctree_control(teststat = "quad",minsplit = 40, minbucket = 20)
tree <- ctree(formula = form, data = train, control = ctrl, method = "class")
```

The desicion tree :

```{r ptk2, echo=FALSE}
plot(tree)
```

Predicting target value and calculating the misclassification error:

```{r ptk3, echo=TRUE}
#Model Testing
out<-predict(tree,test,type = "prob")
status_predicted<- colnames(out)[max.col(out, ties.method = c("first"))] # predicted
status_input<- as.character (test$Class) # actuals
m <- mean (status_input != status_predicted) # misclassification %
print(m)
```

Estimated conditional class probabilities depending on the first split variable. And plotting the same:
```{r ptk4, echo=TRUE}
prob <- predict(tree,test, type = "prob")[,1] + runif(nrow(test), min = -0.01, max = 0.01)
splitvar <- character_split(split_node(node_party(tree)), data = data_party(tree))$name
plot(test[[splitvar]], prob, pch = as.numeric(test$Class), ylab = "Conditional Class Prob.",xlab = splitvar)
abline(v = split_node(node_party(tree))$breaks, lty = 2)
legend(0.15, 0.7, pch = 1:2, legend = levels(test$Class), bty = "n")
```
